{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV77S90jvsO0"
   },
   "source": [
    "## Inference to the Best Explanation (IBE) in Large Language Models (LLMs)\n",
    "\n",
    "IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features. It operates on top of natural language explanations generated by Large Language Models using a combination of hard and soft critique models as a proxy to assess consistency, parsimony, coherence, and uncertainty.\n",
    "\n",
    "<img src=\"figures/ibe.png\" height=\"400\" class=\"center\">\n",
    "\n",
    "## IBE Evaluation Criteria\n",
    "\n",
    "- *Consistency (Hard Critique).* Verify whether the explanation is logically valid. Given a hypothesis, composed of a premise pi, a conclusion ci, and an explanation consisting of a set of statements E =s1,...,si, we define E to be logically consistent if pi ∪ E ⊨ ci. Specifically, an explanation is logically consistent if it is possible to build a deductive proof linking premise and conclusion.\n",
    "\n",
    "- *Parsimony (Soft Critique).* The parsimony principle, also known as Ockham’s razor, favors the selection of the simplest explanation consisting of the fewest elements and assumptions. Adopt two metrics as a proxy of parsimony, namely proof depth, and concept drift.  Concept drift, denoted as Drift, is defined as the\n",
    "number of additional concepts and entities, outside the ones appearing in the hypothesis (i.e., premise and conclusion), that are introduced by the LLM to support the entailment. \n",
    "\n",
    "\\begin{equation}\n",
    "Drift(h) = |Noun_{E} - (Noun_{p} \\cup Noun_{c})\n",
    "\\end{equation}\n",
    "\n",
    "- *Coherence (Soft Critique).* Attempts to measure the logical relations within individual explanatory statements and implications. An explanation can be formally consistent on the surface while still including implausible or ungrounded intermediate assumptions. Coherence evaluate the quality of each intermediate If-Then implication by measuring the entailment strength between the If and Then clauses. To this end, we employ a fine-tuned natural language inference (NLI) model. Let S\n",
    "be a set of explanation steps, where each step s consists of an If-Then statement, s = (Ifs,Thens). For a given step si, let ES(si) denote the entailment score obtained via the NLI model between Ifs and Thens clauses. The step-wise entailment score SWE(S) is then calculated as the averaged sum of the entailment scores across all explanation steps |S|.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{SWE}(S) = \\frac{1}{|S|}\\sum_{i=1}^{|S|} \\text{ES}(s_i)\n",
    "\\end{equation}\n",
    "\n",
    "- *Uncertainty (Soft Critique).* Finally, we consider the linguistic certainty expressed in the generated explanation as a proxy for plausibility. Hedging words such as probably, might be, could be, etc typically signal ambiguity and are often used when the truth condition of a statement is unknown or improbable. Pei and Jurgens (2021) found that the strength of scientific claims in research papers is strongly correlated with the use of direct language. In contrast, they found that the use of hedging language suggested that the veracity of the claim was weaker or highly contextualized. To measure the linguistic certainty we use a fine-tuned sentence-level RoBERTa model.\n",
    "\n",
    "## Results\n",
    "\n",
    "<div>\n",
    "<img src=\"figures/ibe_results.png\" height=\"265\">\n",
    "<img src=\"figures/ibe_results_1.png\" height=\"265\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try with GPT-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Explanations for the hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26061,
     "status": "ok",
     "timestamp": 1731635322587,
     "user": {
      "displayName": "Marco Valentino",
      "userId": "16985191850808964577"
     },
     "user_tz": 0
    },
    "id": "fHzztvy0vsO8",
    "outputId": "c12b0866-a641-43d4-eb35-5ee3d1a5a238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explanation 1:\n",
      "\n",
      "Hypothesis I blew into the baloon.\n",
      "Conlusion The balloon expanded.\n",
      "\n",
      "Step 1: IF someone blows air into a balloon, THEN the balloon will fill with air.\n",
      "Assumption: Blowing air into a balloon introduces air into the balloon's interior.\n",
      "\n",
      "Step 2: IF a balloon fills with air, THEN the pressure inside the balloon increases.\n",
      "Assumption: Adding air to a confined space like a balloon increases the internal pressure.\n",
      "\n",
      "Step 3: IF the pressure inside the balloon increases, THEN the balloon will expand.\n",
      "Assumption: Balloons are made of elastic material that stretches when internal pressure increases.\n",
      "\n",
      "Step 4: Therefore, since you blew into the balloon, air was introduced, increasing the internal pressure and causing the balloon to expand.\n",
      "\n",
      "Explanation 2:\n",
      "\n",
      "Hypothesis I pricked the baloon.\n",
      "Conclusion The balloon expanded.\n",
      "\n",
      "Step 1: IF a balloon is pricked, THEN it will not expand; instead, it will deflate.\n",
      "Assumption: Pricking a balloon creates a hole, causing the air inside to escape, leading to deflation rather than expansion.\n",
      "\n",
      "Step 2: IF a balloon deflates, THEN it cannot expand.\n",
      "Assumption: Expansion requires the balloon to be intact and filled with air, which is not possible if it is deflating.\n",
      "\n",
      "Step 3: Therefore, the hypothesis \"I pricked the balloon\" contradicts the conclusion \"The balloon expanded.\"\n",
      "Assumption: The causal relationship between pricking a balloon and its physical state is such that pricking leads to deflation, not expansion. \n",
      "\n",
      "Conclusion: The provided hypothesis and conclusion are logically inconsistent based on the causal understanding of how balloons behave when pricked.\n"
     ]
    }
   ],
   "source": [
    "# Import the  critique models\n",
    "from critique import CoherenceCritique\n",
    "from critique import ParsimonyCritique\n",
    "from critique import UncertaintyCritique\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Import the generative GPT model\n",
    "from generation.gpt import GPT\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Initialise the generative model (i.e. GPT-4o-mini)\n",
    "with open('config.yaml', 'r') as file:\n",
    "     config = yaml.safe_load(file)\n",
    "     api_key = config.get('gpt-4o', {}).get('api_key')\n",
    "\n",
    "llm = GPT('gpt-4o', api_key)\n",
    "\n",
    "\n",
    "# First hypothesis (change to premise)\n",
    "hypothesis_1 = \"I blew into the baloon.\"\n",
    "conclusion_1 =  \"The balloon expanded.\"\n",
    "\n",
    "# Second hypothesis (change to premise)\n",
    "hypothesis_2 = \"I pricked the baloon.\"\n",
    "conclusion_2 =  \"The balloon expanded.\"\n",
    "\n",
    "# Prompt the model to generate the explanation for the first hypothesis\n",
    "explanation_1 = llm.generate(\n",
    "             model_prompt_dir = 'ibe',\n",
    "             prompt_name = \"generate_explanation_prompt.txt\",\n",
    "             hypothesis = hypothesis_1,\n",
    "             conclusion = conclusion_1\n",
    "         )\n",
    "print(f\"\\nExplanation 1:\\n\\nHypothesis {hypothesis_1}\\nConlusion {conclusion_1}\\n\\n{explanation_1}\")\n",
    "\n",
    "\n",
    "# Prompt the model to generate the explanation for the first hypothesis\n",
    "explanation_2 = llm.generate(\n",
    "             model_prompt_dir = 'ibe',\n",
    "             prompt_name = \"generate_explanation_prompt.txt\",\n",
    "             hypothesis = hypothesis_2,\n",
    "             conclusion = conclusion_2\n",
    "         )\n",
    "print(f\"\\nExplanation 2:\\n\\nHypothesis {hypothesis_2}\\nConclusion {conclusion_2}\\n\\n{explanation_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate explanations via soft critique models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7387,
     "status": "ok",
     "timestamp": 1731635333117,
     "user": {
      "displayName": "Marco Valentino",
      "userId": "16985191850808964577"
     },
     "user_tz": 0
    },
    "id": "TFIqeRU7vsO-",
    "outputId": "613aca8a-d0fa-4bec-c61c-ed3863f1d1b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:862: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:476: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Critique Evaluation\n",
      "\n",
      " ================ Coherence ================\n",
      "\n",
      "Explanation 1:  {'coherence': 0.7369064018130302}\n",
      "Explanation 2:  {'coherence': 0.46557168662548065}\n",
      "Coherence comparision: Explanation 1: 0.7369064018130302 vs. Explanation 2: 0.46557168662548065\n",
      "Explanation 1 is therefore more coherent than Explanation 2.\n",
      "\n",
      "================ Parsimony ================\n",
      "\n",
      "Explanation 1:  {'parsimony': 3}\n",
      "Explanation 2:  {'parsimony': 3}\n",
      "\n",
      "Parsimony comparision: Explanation 1: 3 vs. Explanation 2: 3\n",
      "Explanation 2 is therefore more parsimonious than Explanation 1.\n",
      "\n",
      "================ Uncertainty ================\n",
      "\n",
      "Explanation 1:  {'uncertainty': 0.9982295831044515}\n",
      "Explanation 2:  {'uncertainty': 1.0043059190114338}\n",
      "\n",
      "Uncertainty comparision: Explanation 1: 0.9982295831044515 vs. Explanation 2: 1.0043059190114338\n",
      "Explanation 2 is therefore more uncertain than Explanation 1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialise the soft critique models\n",
    "coherence = CoherenceCritique()\n",
    "parsimony = ParsimonyCritique()\n",
    "uncertainty = UncertaintyCritique()\n",
    "\n",
    "print(\"Soft Critique Evaluation\")\n",
    "# Calculate and display soft critique scores\n",
    "\n",
    "# Coherence Metrics\n",
    "exp1_coherence = coherence.critique(explanation=explanation_1)\n",
    "exp2_coherence = coherence.critique(explanation=explanation_2)\n",
    "\n",
    "print(\"\\n ================ Coherence ================\\n\")\n",
    "\n",
    "print(\"Explanation 1: \", exp1_coherence)\n",
    "print(\"Explanation 2: \", exp2_coherence)\n",
    "\n",
    "print(f\"Coherence comparision: Explanation 1: {exp1_coherence['coherence']} vs. Explanation 2: {exp2_coherence['coherence']}\")\n",
    "\n",
    "if exp1_coherence['coherence'] > exp2_coherence['coherence']:\n",
    "    print(\"Explanation 1 is therefore more coherent than Explanation 2.\")\n",
    "else:\n",
    "    print(\"Explanation 2 is the most coherente than Explanation 1.\")\n",
    "\n",
    "# Parsimony Metrics\n",
    "exp1_parsimony = parsimony.critique(hypothesis_1, conclusion_1, explanation_1)\n",
    "exp2_parsimony = parsimony.critique(hypothesis_2, conclusion_2, explanation_2)\n",
    "\n",
    "print(\"\\n================ Parsimony ================\\n\")\n",
    "\n",
    "print(\"Explanation 1: \", exp1_parsimony)\n",
    "print(\"Explanation 2: \", exp2_parsimony)\n",
    "\n",
    "print(f\"\\nParsimony comparision: Explanation 1: {exp1_parsimony['parsimony']} vs. Explanation 2: {exp2_parsimony['parsimony']}\")\n",
    "\n",
    "if exp1_parsimony['parsimony'] < exp2_parsimony['parsimony']:\n",
    "    print(\"Explanation 1 is therefore more parsimonious than Explanation 2.\")\n",
    "else:\n",
    "    print(\"Explanation 2 is therefore more parsimonious than Explanation 1.\")\n",
    "\n",
    "# Uncertainty Metrics\n",
    "exp1_uncertainty = uncertainty.critique(explanation=explanation_1)\n",
    "exp2_uncertainty = uncertainty.critique(explanation=explanation_2)\n",
    "\n",
    "print(\"\\n================ Uncertainty ================\\n\")\n",
    "\n",
    "print(\"Explanation 1: \", exp1_uncertainty)\n",
    "print(\"Explanation 2: \", exp2_uncertainty)\n",
    "\n",
    "print(f\"\\nUncertainty comparision: Explanation 1: {exp1_uncertainty['uncertainty']} vs. Explanation 2: {exp2_uncertainty['uncertainty']}\")\n",
    "\n",
    "if exp1_uncertainty['uncertainty'] > exp2_uncertainty['uncertainty']:\n",
    "    print(\"Explanation 1 is therefore more uncertain than Explanation 2.\")\n",
    "else:\n",
    "    print(\"Explanation 2 is therefore more uncertain than Explanation 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try with GPT-3.5-Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3122,
     "status": "ok",
     "timestamp": 1731635469816,
     "user": {
      "displayName": "Marco Valentino",
      "userId": "16985191850808964577"
     },
     "user_tz": 0
    },
    "id": "UmW-kKd1iliL",
    "outputId": "5a2e31fd-ac53-4b01-b3c4-3722d101d03c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explanation 1:\n",
      "\n",
      "Hypothesis I blew into the baloon.\n",
      "Conlusion The balloon expanded.\n",
      "\n",
      "Step 1: IF air is blown into a balloon, THEN the balloon can expand.\n",
      "Assumption: Blowing air into a balloon increases the air pressure inside, causing it to expand.\n",
      "\n",
      "Step 2: Therefore, since you blew into the balloon, the balloon expanded as a result of the increased air pressure inside.\n",
      "\n",
      "Explanation 2:\n",
      "\n",
      "Hypothesis I pricked the baloon.\n",
      "Conclusion The balloon expanded.\n",
      "\n",
      "Step 1: IF a balloon is pricked, THEN the air inside the balloon can escape.\n",
      "Assumption: When a balloon is pricked, it creates a hole through which the air inside can exit.\n",
      "\n",
      "Step 2: IF the air inside a balloon escapes, THEN the pressure inside the balloon decreases.\n",
      "Assumption: As the air escapes from the balloon, the pressure inside the balloon decreases.\n",
      "\n",
      "Step 3: IF the pressure inside a balloon decreases, THEN the balloon can expand.\n",
      "Assumption: A decrease in pressure inside a balloon can cause the balloon to expand as the external pressure exceeds the internal pressure.\n",
      "\n",
      "Step 4: Therefore, when you pricked the balloon, causing the air to escape and the pressure inside to decrease, the balloon expanded as a result.\n"
     ]
    }
   ],
   "source": [
    "# Import the  critique models\n",
    "from critique import CoherenceCritique\n",
    "from critique import ParsimonyCritique\n",
    "from critique import UncertaintyCritique\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Import the generative GPT model\n",
    "from generation.gpt import GPT\n",
    "import yaml\n",
    "\n",
    "\n",
    "# Initialise the generative model (i.e. GPT-4o)\n",
    "with open('config.yaml', 'r') as file:\n",
    "     config = yaml.safe_load(file)\n",
    "     api_key = config.get('gpt-3.5-turbo', {}).get('api_key')\n",
    "\n",
    "llm = GPT('gpt-3.5-turbo', api_key)\n",
    "\n",
    "\n",
    "# First hypothesis (change to premise)\n",
    "hypothesis_1 = \"I blew into the baloon.\"\n",
    "conclusion_1 =  \"The balloon expanded.\"\n",
    "\n",
    "# Second hypothesis (change to premise)\n",
    "hypothesis_2 = \"I pricked the baloon.\"\n",
    "conclusion_2 =  \"The balloon expanded.\"\n",
    "\n",
    "# Prompt the model to generate the explanation for the first hypothesis\n",
    "explanation_1 = llm.generate(\n",
    "             model_prompt_dir = 'ibe',\n",
    "             prompt_name = \"generate_explanation_prompt.txt\",\n",
    "             hypothesis = hypothesis_1,\n",
    "             conclusion = conclusion_1\n",
    "         )\n",
    "print(f\"\\nExplanation 1:\\n\\nHypothesis {hypothesis_1}\\nConlusion {conclusion_1}\\n\\n{explanation_1}\")\n",
    "\n",
    "\n",
    "# Prompt the model to generate the explanation for the first hypothesis\n",
    "explanation_2 = llm.generate(\n",
    "             model_prompt_dir = 'ibe',\n",
    "             prompt_name = \"generate_explanation_prompt.txt\",\n",
    "             hypothesis = hypothesis_2,\n",
    "             conclusion = conclusion_2\n",
    "         )\n",
    "print(f\"\\nExplanation 2:\\n\\nHypothesis {hypothesis_2}\\nConclusion {conclusion_2}\\n\\n{explanation_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate explanations via soft critique models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6764,
     "status": "ok",
     "timestamp": 1731635479444,
     "user": {
      "displayName": "Marco Valentino",
      "userId": "16985191850808964577"
     },
     "user_tz": 0
    },
    "id": "j6Jb-1KgipEr",
    "outputId": "cff888ec-b8d8-4a29-df0a-b6a957ec3f7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Critique Evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ================ Coherence ================\n",
      "\n",
      "Explanation 1:  {'coherence': nan}\n",
      "Explanation 2:  {'coherence': 0.39371824637055397}\n",
      "Coherence comparision: Explanation 1: nan vs. Explanation 2: 0.39371824637055397\n",
      "Explanation 2 is the most coherente than Explanation 1.\n",
      "\n",
      "================ Parsimony ================\n",
      "\n",
      "Explanation 1:  {'parsimony': 1}\n",
      "Explanation 2:  {'parsimony': 3}\n",
      "\n",
      "Parsimony comparision: Explanation 1: 1 vs. Explanation 2: 3\n",
      "Explanation 1 is therefore more parsimonious than Explanation 2.\n",
      "\n",
      "================ Uncertainty ================\n",
      "\n",
      "Explanation 1:  {'uncertainty': 1.010850429534912}\n",
      "Explanation 2:  {'uncertainty': 1.0850576559702554}\n",
      "\n",
      "Uncertainty comparision: Explanation 1: 1.010850429534912 vs. Explanation 2: 1.0850576559702554\n",
      "Explanation 2 is therefore more uncertain than Explanation 1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialise the soft critique models\n",
    "coherence = CoherenceCritique()\n",
    "parsimony = ParsimonyCritique()\n",
    "uncertainty = UncertaintyCritique()\n",
    "\n",
    "print(\"Soft Critique Evaluation\")\n",
    "# Calculate and display soft critique scores\n",
    "\n",
    "# Coherence Metrics\n",
    "exp1_coherence = coherence.critique(explanation=explanation_1)\n",
    "exp2_coherence = coherence.critique(explanation=explanation_2)\n",
    "\n",
    "print(\"\\n ================ Coherence ================\\n\")\n",
    "\n",
    "print(\"Explanation 1: \", exp1_coherence)\n",
    "print(\"Explanation 2: \", exp2_coherence)\n",
    "\n",
    "print(f\"Coherence comparision: Explanation 1: {exp1_coherence['coherence']} vs. Explanation 2: {exp2_coherence['coherence']}\")\n",
    "\n",
    "if exp1_coherence['coherence'] > exp2_coherence['coherence']:\n",
    "    print(\"Explanation 1 is therefore more coherent than Explanation 2.\")\n",
    "else:\n",
    "    print(\"Explanation 2 is the most coherente than Explanation 1.\")\n",
    "\n",
    "# Parsimony Metrics\n",
    "exp1_parsimony = parsimony.critique(hypothesis_1, conclusion_1, explanation_1)\n",
    "exp2_parsimony = parsimony.critique(hypothesis_2, conclusion_2, explanation_2)\n",
    "\n",
    "print(\"\\n================ Parsimony ================\\n\")\n",
    "\n",
    "print(\"Explanation 1: \", exp1_parsimony)\n",
    "print(\"Explanation 2: \", exp2_parsimony)\n",
    "\n",
    "print(f\"\\nParsimony comparision: Explanation 1: {exp1_parsimony['parsimony']} vs. Explanation 2: {exp2_parsimony['parsimony']}\")\n",
    "\n",
    "if exp1_parsimony['parsimony'] < exp2_parsimony['parsimony']:\n",
    "    print(\"Explanation 1 is therefore more parsimonious than Explanation 2.\")\n",
    "else:\n",
    "    print(\"Explanation 2 is therefore more parsimonious than Explanation 1.\")\n",
    "\n",
    "# Uncertainty Metrics\n",
    "exp1_uncertainty = uncertainty.critique(explanation=explanation_1)\n",
    "exp2_uncertainty = uncertainty.critique(explanation=explanation_2)\n",
    "\n",
    "print(\"\\n================ Uncertainty ================\\n\")\n",
    "\n",
    "print(\"Explanation 1: \", exp1_uncertainty)\n",
    "print(\"Explanation 2: \", exp2_uncertainty)\n",
    "\n",
    "print(f\"\\nUncertainty comparision: Explanation 1: {exp1_uncertainty['uncertainty']} vs. Explanation 2: {exp2_uncertainty['uncertainty']}\")\n",
    "\n",
    "if exp1_uncertainty['uncertainty'] > exp2_uncertainty['uncertainty']:\n",
    "    print(\"Explanation 1 is therefore more uncertain than Explanation 2.\")\n",
    "else:\n",
    "    print(\"Explanation 2 is therefore more uncertain than Explanation 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison \n",
    "\n",
    "The explanations generated by GPT-4-o for this example have a better \"separation\" than the ones generated by GPT-3.5-turbo.\n",
    "\n",
    "GPT-4o:\n",
    "\n",
    "- Coherence comparision:  Explanation 1: 0.7090832414105535 vs. Explanation 2: 0.29190194606781006\n",
    "- Parsimony comparision:  Explanation 1: 1 vs. Explanation 2: 12\n",
    "- Uncertainty comparision: Explanation 1: 0.999363899230957 vs. Explanation 2: 1.5216406186421714\n",
    "\n",
    "\n",
    "GPT-3.4-Turbo:\n",
    "\n",
    "- Coherence comparision: Explanation 1: 0.871249190531671 vs. Explanation 2: 0.06694453046657145\n",
    "- Parsimony comparision: Explanation 1: 3 vs. Explanation 2: 6\n",
    "- Uncertainty comparision: Explanation 1: 1.0230472882588706 vs. Explanation 2: 2.125597596168518"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1beM923HvLSUEf6eJ-bSOaqx9ICx7Fr_b",
     "timestamp": 1730906628931
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
